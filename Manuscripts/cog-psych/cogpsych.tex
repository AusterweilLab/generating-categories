%!TEX output_directory = latex_out/

\documentclass[12pt]{article}
\usepackage[letterpaper, margin=1in, headheight=15pt]{geometry}
\usepackage{amsmath}
\usepackage{setspace}
\usepackage{pgfplots}
\usepackage{fancyhdr}
\usepackage{csquotes}
\usepackage{todonotes}
\usepackage{verbatim}
\usepackage[toc]{appendix}
\usepackage[natbibapa]{apacite}


% set up PGF
\pgfplotsset{compat=1.6}
\newcommand\inputpgf[2]{{
\let\pgfimageWithoutPath\pgfimage
\renewcommand{\pgfimage}[2][]{\pgfimageWithoutPath[##1]{#1/##2}}
\input{#1/#2}
}}

% set up notes-- different backgrounds for Nolan and Joe!
\newcommand\nbcnote[1]{\todo[inline, backgroundcolor = yellow]{\textbf{NBC}: #1}}
\newcommand\jlanote[1]{\todo[inline, backgroundcolor = lime]{\textbf{JLA}: #1}}

% set up header % 
\pagestyle{fancy}
\fancyhf{} % sets both header and footer to nothing
\renewcommand{\headrulewidth}{0pt}
\lhead{RUNNING HEAD: Similarity and Contrast in Concept Generation}
\fancyhead[R]{\thepage}


% author info:
% https://www.elsevier.com/journals/cognitive-psychology/0010-0285/guide-for-authors


\begin{document}

% ------- TITLE PAGE ------- %
\begin{center}
\hfill
\\[1in]

Creating Something Different: Similarity and Contrast in Concept Generation.


\vfill

Nolan Conaway\textsuperscript{1}, 
Kenneth J. Kurtz\textsuperscript{2}, 
and Joseph L. Austerweil\textsuperscript{1}
\\[\baselineskip]
\textsuperscript{1}University of Wisconsin-Madison, Department of Psychology, Madison, WI, USA
\textsuperscript{2}Department of Psychology, Binghamton University, Binghamton, NY, USA
\\[1in]

\vfill

Author Note

Correspondence concerning this article should be addressed to: 
Joseph Austerweil, 1202 West Johnson Street, Madison, WI 53706.
E-mail: austerweil@wisc.edu

\end{center}
\clearpage


% ------- ABSTRACT PAGE ------- %
\doublespacing
\section*{Abstract}

\jlanote{Joe! Did you know i made a command for you to enter in comments like this?? }

\jlanote{This is awesome!!!}

\begin{verbatim}
    \jlanote{Some note}
\end{verbatim}


\setlength\parindent{0.5in}
\textit{Keywords}: categorization, concepts, creativity, generation
\clearpage


% ------- BEGIN! ------- %
\begin{flushleft}

\section{Introduction}
\setlength\parindent{0.5in}

Creativity, innovation, imagination, and the creation of new ideas are among the most fascinating, important and difficult human capabilities to study. They are fascinating and important capabilities because they have produced some of the greatest scientific breakthroughs that revolutionized the world. Unfortunately, scientific breakthroughs are rare and the result of a great deal of cognitive effort, which it difficult to investigate the underlying cognitive representations and processes responsible for them using standard experimental methodology or formalize in computational models. Although we tend to focus on the most salient products of these processes (e.g., scientific breakthroughs), every person is likely to have generated a novel sentence, thought, and/or drawing.\footnote{\jlanote{We need citations/examples for this. In a compositional system, it's pretty clear this has to be the case, but I'd like to use someone else as a reference rather than arguing this myself.}} This observation provides a fantastic opportunity: By examining how people create new concepts in a domain amenable to formalization in computational models, we can investigate (some aspects of) creativity, innovation, and imagination scientifically.

%The creation of novel concepts and ideas is a highly intriguing -- yet infrequently studied -- topic of research in human cognition. The creative use of conceptual knowledge is, for example, a core element of scientific investigation, wherein the generation of new ideas is critical to designing experiments and explaining observations. However, due to its complexity, little is presently known about the cognitive processes underlying the generation of new concepts. 

Towards the aim of providing a formal account of the cognitive mechanisms involved in creativity and innovation, we explore a simplified form of creative cognition that can be studied using standard behavioral methodology and is amenable to formal modeling: category generation. In a category generation task, participants are given some background knowledge about a domain (a cover story and/or learning one or more categories in the domain) and then are asked to generate another category in the domain \citep{jern2013probabilistic,ward1994structured}. Unlike creativity and innovation, categorization is well studied from a breadth of perspectives, including behavioral, neural, comparative, and computational  \citep{kurtz2015human,mack2013,margolis2015,pothoswills2011}. By analyzing a creative task using formal tools from the categorization literature, we can formalize some theories of creativity and test them empirically in a more rigorous manner. 

Previous work has focused on one key aspect of category generation: People tend to create new categories that have similar {\em statistical regularities} to previously known categories \citep{jern2013probabilistic,ward1994structured}. Although this is an important characteristic of generating new categories, it cannot be the only one. Taken to the extreme, the best ``new'' category in terms of having the same statistical regularities to other categories would be one that is identical to a prototypical previously observed category (and thus, not new at all). 

In this article, we propose a new constraint that guides category generation: ``being different'' or contrasting from other categories in the relevant domain. To some extent, this constraint has been implicitly assumed in much previous work: the idea that a new category should be ``different'' is vague, as there are many ways it could be different from a previously observed category. Building on the largely successful exemplar modeling framework \citep{medin1978context,nosofsky1984choice,nosofsky1986attention}, we propose a novel exemplar model of category generation, {\em Producing Alike and Contrasting Knowledge using Exemplar Representations} (PACKER), formalizing how new categories should differ from previous categories. This model makes novel predictions about how contrast affects category generation, which we test using behavioral experiments.

The outline of the article is as follows. First we describe previous empirical work on the topic of category generation, as well as the computational approaches studied in those reports. Then we describe our novel computational model, which is designed to generate categories that systematically differ from existing categories in the domain. We present two experiments demonstrating strong and systematic effects of category contrast on creative generation, and we qualitatively and quantitatively analyze the performance of each model in capturing human category generation. We conclude with a discussion of the implications of our results for categorization and creative cognition, and directions for future work.

\section{Prior work}

Much of what we know about concept generation comes from the foundational literature on creative cognition. In a classic series of reports, Ward and colleagues \citep{ward1995s,ward1994structured,marsh1999inadvertent,ward2002role,smith1993constraining} established that category generation is highly constrained by prior knowledge: Generated categories tend to consist of features observed in known categories, and they tend to exhibit the distributional properties as found in known categories. In a classic study, \cite{ward1994structured} asked participants to generate new species of alien animals by drawing and describing members of the species. People tended to generate species with the same features as on Earth (e.g., eyes, legs, wings), and possessing the same feature correlations as on Earth (e.g., feathers co-occur with wings). Likewise, aliens drawn from the same species tended to share more features with one another compared to members of opposite species. 

Much of the work from this area \citep[e.g.,][]{smith1993constraining,marsh1999inadvertent} focuses on how sample cues (such as an example of a species generated by other participants) can drastically diminish creativity. However, the broader set of observations made by Ward and colleagues provide a great deal of insight into the nature of creative generation. They indicate that people rely strongly on prior knowledge in the creative process, and people generate concepts in accord with what they already know. Theoretical accounts of these effects have primarily been grounded within the categorization literature. For example, the predominant ``Path of Least Resistance'' account \citep[see][]{ward1994structured,ward1995s,ward2002role} proposes that, when generating a new species of animal, people retrieve from memory a known subcategory of animals (e.g., \textit{bird}, \textit{dog}, \textit{horse}), and simply change some of the features to make something new. People were thought to change only features that are not characteristic of the retrieved category (e.g., if \textit{bird} was retrieved, the presence of \textit{wings} would not change, but \textit{color} might). This theory incorporates elements of the highly influential basic-level categories framework \citep{rosch1975cognitive,rosch1976basic}, as well as the exemplar view \citep{medin1978context,brooks1978nonanalytic}. Problematically, however, the experimental paradigms employed in these studies were relatively uncontrolled, precluding the development of formal approaches. Specifically, participants in these studies are typically asked to draw exemplars of novel categories by hand, and so it is difficult to apply formal models to the data.

\jlanote{The issue isn't prior knowledge actually -- Bayesian models can handle that gracefully -- it's the rich format of participant responses being 2-d sketches}

\cite{jern2013probabilistic} recently showed that creative generation could be studied in a more controlled manner through the well-developed methods of an artificial categorization paradigm \citep[see][for a review]{kurtz2015human}. In Experiments 3 and 4 of their article, participants were exposed to members of experimenter-defined categories of "crystals" varying in size, hue, and saturation. Following a training phase during which the experimenter-defined categories were learned, participants were asked to generate novel categories of crystals. In a finding mirroring that of the \cite{ward1994structured} studies, \citeauthor{jern2013probabilistic} found that participants generated categories with the same distributional properties as the experimenter-defined categories: for example, after training on categories with a positive correlation between the size and saturation features (larger sized crystals were more saturated), participants generated novel categories with the same positive correlation. This finding is notable, as it demonstrates that category generation can be studied in a well-known and highly controlled experimental paradigm. Note that generated exemplars for the novel category automatically contrasted from previous categories because the participants selected the size and saturation, but the exemplar's hue was set (and fixed) to a novel value. As such, hue was ignored in their analyses and may have satisfied any constraints contrast may have on category generation. 

\jlanote{It is hierarchical Bayesian, not Bayesian hierarchical.}

The authors evaluated the predictions of several formal models on their data. Most notably, they showed that a hierarchical Bayesian sampling model provided the strongest account. Their model views observed examples as samples from an underlying category distribution, describing the location of the category in the space, as well as how it varies along each feature. In turn, each category is viewed as a sample from an underlying \textit{domain} distribution, specifying distributional commonalities among the observed categories. Generated categories are thought to stem from the same domain distribution as observed categories, thus the distributional properties of observed categories will be preserved within the generated category.

% Here's what it used to be:
% Their model represents categories as multivariate normal distributions within the space, and from exposure to category members the model induces a domain-wide representation of the distributional commonalities among the observed categories. When generating a new category, the model samples a new distribution from the domain, and thus the distributional properties of observed categories will be preserved within the generates category.

% \jlanote{Yeah, Here's how I would this section should be organized. Top is general behavioral findings/phenomena. Then subsections for each theory, which first is described in broad theoretical terms and then how that is formalized into a model}

\cite{jern2013probabilistic} additionally tested a "copy-and-tweak" model that broadly resembles the earlier "Path of Least Resistance" account. The core proposal is that participants generate new items by copying stored examples from memory and tweaking them to generate something new. The copy-and-tweak model differs from the path of least resistance account in that it notably omits the hierarchical organization of categories, as well as selectivity in which features are changed. Instead, their copy-and-tweak model corresponds to a direct exemplar-similarity approach \citep[e.g.,][]{nosofsky1984choice,nosofsky1986attention}: The model generates new items according to their similarity to known members of the target category. This model provided a poor account of the observed data. Indeed, the experiments devised by \citeauthor{jern2013probabilistic} were specifically designed to challenge this model. However, its application is notable as a first step toward explaining category generation using well-known formal approaches from the categorization literature.


\subsection{Something Different: A Role For Contrast}

It is worth noting that the literature reviewed above provides a somewhat limited view of category generation. The main concern of the published experiments has been on the distributional correspondences between learned and generated categories, and as a result most of the modeling efforts have been towards explaining those effects. In this paper, we investigate another important constraint on the creative use of conceptual knowledge: category contrast. To generate a novel concept, individuals must produce something that is in some capacity \textit{different} from what they already know. Thus, contrast is a fundamental constraint on creative generation: new concepts must firstly be different from existing ones. 

% Joe says he'll do a pass on this language later:
Although it is evident that people are \textit{capable} of creating new concepts and categories, it is not entirely clear how new concepts are systematically made different from what is already known. The hierarchical sampling model developed by \cite{jern2013probabilistic} assumes that differences between generated category are only due to random variation. The model only proposes that generated categories are sampled from the same underlying domain distribution as observed categories, and will thus share a common distributional structure. The authors do not make predictions about the \textit{location} of the category within the domain (the perceptual instantiation of category members). Indeed, under a strict interpretation of their model, the most probable new category to be generated is located in {\em exactly} the same location and distributional information as the given category. However, this is not simply an issue with their model -- Hierarchical Bayesian models assume the same underlying distribution generates all of the categories and thus, any differences between categories is due to {\em noise} and should not be {\em systematic}. The best a hierarchical Bayesian model can do at capturing contrast is to assume that the new category is placed uniformly at random over stimulus space. This defeats the purpose of a hierarchy as new category locations do not depend on the locations of previously observed categories.


The copy-and-tweak model tested by \cite{jern2013probabilistic} also claims little about how generated categories should contrast with what is already known. In their simulations, the model was only tested on generation after the learner had been exposed to members of the target category, and so the model's ability to generate a new category was not evaluated. However, the model's generation is based exclusively on similarity to known members of the target category; when there are no members of the target category, generation is presumably random.



% \section{Formalizing category generation}
% \jlanote{this should come before the studies as formalizations of the different theories (some details may be delegated to Appendices). I like trying to integrate them with the verbal description of the theories rather than having a separate computational modeling section. But the latter works for sure and I'm not sure if the former will flow well...}


\subsection{The PACKER Model}

\label{section:PACKER-definition}

As noted above, the constraint that new concepts should differ from what is already known has been largely been overlooked in previous work. This is no doubt in part due to the vague definition of what it means for a concept to be ``different'': A generated category may be different from what is already known in any number of respects. Towards formalizing the role of contrast in creative generation, we developed the PACKER  ({\em Producing Alike and Contrasting Knowledge using Exemplar Representations}) model which explains category generation as a balance between two fundamental constraints: generated categories should not be similar to known categories, and exemplars within each category should be similar to one another. These ideas are implemented within the well-studied exemplar framework: the PACKER model is an extension of the influential Generalized Context Model of category learning \citep[GCM;][]{nosofsky1984choice,nosofsky1986attention}. 

\nbcnote{Maybe note something about how PACKER's basis in the exemplar view means that we're explaining category generation using well known ideas, and that exemplar models are viewed as highly principled.}

\jlanote{Yeah, I think this would make a nice subsection in the general discussion.}

Both PACKER and the GCM simulate categorization under the assumption that learners represent categories as a collection of exemplars, corresponding to the labeled stimuli they have observed. The exemplars are encoded within a $k$-dimensional psychological space, and model performance is based on the amount of similarity between the item to be categorized and the stored exemplars. Similarity between two examples, $s\left(x_i, x_j\right)$, is computed as an inverse exponential function of distance \citep[following][]{attneave1950,shepard1957stimulus,shepard1987toward}:
\jlanote{attneave was first (though he was inspired by hypotheses made by householder and others, so it may date back even earlier). he framed it in terms of sum of log distances, but it's equivalent. funnily he said the biggest issue is violations of the triangle inequality via examples attributed to James. i can send you the article if you want....}
\begin{equation}
s\left(x_i,x_j\right) = \exp \left\{ -c \left[\sum_{k}{ w_k \left| x_{ik} - x_{jk} \right|^r }\right]^{1/r} \right\}
\label{eq:similarity}
\end{equation}
% 
where $w_k$ is the attention weighting of dimension $k$ ($w_k \geq 0$ and $\sum_k{w_k} = 1$), accounting for the relative importance of each dimension in similarity calculations, and $c$ ($c>0$) is a specificity parameter controlling the spread of exemplar generalization. For simplicity, attention will be distributed uniformly in our simulations (unless otherwise noted). The value of $r$ depends on the nature of the experimental conditions being simulated: $r=1$ is appropriate for separable dimensions, whereas $r=2$ is appropriate for integral dimensions \citep[see][]{shepard1964attention,garner1974processing}. In our simulations, we set $r=1$ due to the separable nature of the stimulus dimensions used in our experiments (see Figure \ref{fig:e1-conditions}).


PACKER (as well as its name) was in part inspired by earlier work from the categorization literature \citep{hidaka2011packing}. They argued that natural categories "pack" the values of features such that they fill the space with as much distance apart from one another, while maintaining items within the same category close together. Inspired by this idea, PACKER proposes that generation is constrained by both similarity to members of the target category (the category in which a stimulus is being generated) as well as similarity to members of other categories: the most desirable generation candidates are similar to members of the target category and not similar to members of contrast categories. This is achieved by aggregating similarity across known exemplars differently according to class membership. The aggregated similarity $a$ between generation candidate $y$ and stored exemplars $x$ is given by:

\begin{equation}
a(y, x) = \sum_j{f(x_j) s(y, x_j)}
\end{equation}
% 
where $f(x_j)$ is a function specifying each exemplar's contribution to generation. PACKER sets $f(x_j)$ depending on exemplar $x_j$'s category membership: $f(x_j) = \phi$ if $x_j$ is a member of a contrast category, and $f(x_j) = \gamma$ if $x_j$ is a member of the target category. $\phi$ and $\gamma$ are free parameters ($-\infty \leq \phi, \gamma \leq \infty$) controlling the contribution of contrast- and target-category similarity, respectively. Larger absolute values result in greater consideration of those exemplars, with values of 0 eliminating their effect. A negative value for $f(x_j)$ produces a `repelling' effect (exemplars are less likely to be generated nearby $x_j$). Conversely, a positive value for $f(x_j)$ produces an `attracting' effect (exemplars are more likely to be generated nearby $x_j$). 

\begin{figure}
	\begin{center}
		\inputpgf{figs/}{packer-examples.pgf}
		\caption{PACKER generation of a category `B' example, following exposure to one member of category `A' and one member of category `B'. Predictions are shown for three different parameterizations: \textit{(a)} Predictions based on contrast similarity only. \textit{(b)} Predictions based on target similarity only.  \textit{(c)} Predictions with both constraints considered.}
		\label{fig:packer-examples}
	\end{center}
\end{figure}

\jlanote{Is our parameterization overdetermined? In other words, are there multiple parameter values that yield equivalent models? It seems like the sum of $\gamma$ and $\phi$ are related to $\theta$. Is that right? If this is the case, we need to either remove a parameter (if it is completely redundant) or place a constraint (e.g., $\gamma + \phi $ = 1)...}

As noted above, PACKER proposes that new categories should be different from existing categories, and same-category exemplars should be similar to one another. This is realized when $\phi < 0$, and $\gamma > 0$. Negative $\phi$ values encourage $y$ to be distant from contrast categories (as similarity to contrast category exemplars are subtracted during aggregation). Positive $\gamma$ values encourage $y$ to be close to other exemplars of the target category. When $|\phi| = \gamma$, the repulsion effect from contrast categories is equal to the attraction effect to the target category. See Figure \ref{fig:packer-examples} for an illustration of how these parameters are used.

However, PAKCKER is only one possible exemplar-based account of category generation within our proposed framework. PACKER places specific constraints on the possible parameter values (i.e., $\phi <0$ and $\gamma > 0$). Other exemplar-based category generation models with drastically different behavior can be formalized in this framework by changing the constrains on the $\phi$ and $\gamma$ parameters. Furthermore, by fitting these parameters to a dataset, one can describe the relative roles of between-class contrast and within-class similarity in generation. For example, as will be discussed in more detail below, PACKER is formally equivalent to the copy-and-tweak model proposed by \cite{jern2013probabilistic} when $\phi = 0$ and $\gamma = 1$. Likewise, when $\phi < 0$ and $\gamma = 0$, PACKER represents a contrast-only generation mode, relying exclusively on contrast when generating new categories. Finally, when $\phi = \gamma < 0$, PACKER represents a ``pure-packing'' approach, placing exemplars in any unoccupied area of the domain. Thus, beyond specifying PACKER's core proposal, the $\phi$ and $\gamma$ allow the us to address a wide variety of qualitatively distinct generation strategies.

\nbcnote{That last paragraph was added since your last read, joe!}

The probability that a given candidate $y$ will be generated is evaluated using an Exponentiated \citet{luce1977choice} choice rule. Candidates with greater values of $a$ are more likely to be generated than candidates with smaller values:
% 
\begin{equation}
p(y) = \dfrac
{ \exp  \left \{ \theta \cdot a \left( y, x \right) \right \} } 
{ \sum_i{ \exp  \left \{ \theta \cdot a \left( y_i, x \right) \right\}  } }
\label{eq:packer-choice}
\end{equation}
% 
where $\theta$ ($\theta \geq 0$) is a free parameter controlling response determinism. 

It is worth noting that PACKER represents just one of many possible models that incorporate contrast in category generation. For example, although it may be possible to extend \cite{jern2013probabilistic}'s hierarchical sampling model to include contrast, but this mechanism would be at odds with a conjugate hierarchical Bayesian framework. In fact, we will discuss integrating our approaches in the General Discussion. In contrast, these ideas emerge naturally from the exemplar view: we have not modified any of the core elements of the GCM in defining the PACKER model, we simply aggregated similarity slightly differently. \nbcnote{That line about importance sampling can go here.} \jlanote{I think I'll write a subsection about it/include it in the subsection on future directions. I may do a bit of math to show how it would be possible too.}

\subsubsection{Relation Between PACKER and Copy-And-Tweak}
\label{section:copytweak-packer}
The PACKER model is some sense similar to the copy-and-tweak model reported by \cite{jern2013probabilistic}: both models are exemplar-based, and both models generate new items according to their similarity to known members of the target class. PACKER diverges from the copy-and-tweak model by including a contrast mechanism, enabling generation according to dissimilarity to members of opposing categories. By consequence, copy-and-tweak can be realized as a parameterization of the PACKER model that is insensitive to category contrast. Specifically, when $\phi = 0$ and $\gamma = 1$ (see Figure \ref{fig:packer-examples}, panel B), PACKER is not influenced by similarity to members of opposite categories, and is mathematically equivalent to a copy-and-tweak approach. 

In this paper, we report simulations using this copy-and-tweak model. This model fits within the exemplar-based category generation framework defined above, under the constraints that $\phi = 0$ and $\gamma = 1$, and is a continuous-dimension adaptation of the model tested by \cite{jern2013probabilistic}. By formalizing a model family where PACKER and copy-and-tweak are different parameterizations of models within the same framework, the comparison between PACKER and copy-and-tweak provides a test of the explanatory value of the contrast mechanism: the account provided by copy-and-tweak will only equal that of PACKER if the contrast mechanism does not offer an advantage (i.e., if $\phi \neq 0$ significantly improves model fits).


\subsection{Synopsis and Prognosis}

Research on the creative generation of novel concepts has focused on the finding that generated categories tend to possess distributional commonalities with known categories. However, a fundamental goal of concept generation is to create something \textit{new} (i.e., different from what is already known). The manner in which generated categories differ from known ones is, nonetheless, poorly understood: Existing theories do not make strong predictions about how creatively generated concepts should systematically differ from existing ones. Above, we introduced a novel, exemplar-based model formalizing the roles of similarity and contrast in creative generation.

In the sections below, we present two experiments demonstrating systematic effects of category contrast on creative generation. Our experiments are based on \cite{jern2013probabilistic}'s paradigm: participants are first exposed to a single, experimenter-defined category, and are then asked to generate members of a new category. We then we report formal simulations comparing PACKER's account of our results to that of the hierarchical sampling and copy-and-tweak models developed by \cite{jern2013probabilistic}.


\section{Experiment 1}

To begin our investigation, we developed an artificial, two dimensional domain of squares, varying in color and size (see Figure \ref{fig:e1-conditions}, panel A). To foreshadow slightly, effects of contrast are strong and can be widely observed in category generation data. Accordingly, we began by testing the strength of these effects under a variety of learning conditions. This was achieved by training participants on categories possessing qualitatively different distributional structures, as shown in Figure \ref{fig:e1-conditions}. 

\jlanote{Can you make a note here about the counterbalancing we did and what that means about the structures? E.g., Does it mean cluster really is tested in all four quadrants of the currently depicted axes and both rows and columns are tested? I thought that was the case, but I wanted to double check and have you write it in a manner that is definitely what we did.}

Panels B-D of Figure \ref{fig:e1-conditions} show the locations of exemplars belonging to the experimenter-defined categories (`A', or `Alpha') that participants were assigned to learn about prior to generating a new category. In the `Cluster' type, category A is a tight cluster of examples in the space. Perceptually instantiated, the members of category A might, for example, be large and dark in color. In the `Row' type, category A has a row pattern across the space, varying along one feature but not the other. Thus, its members might all be dark in color but would vary in size. Finally, in the `XOR' type, the experimenter-defined category consists of two clusters separated in opposite corners of the space category, conforming to the exclusive-or logical structure (e.g., members are small and dark or large and light). 

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e1-conditions.pgf}
    \caption{Stimulus domain and category types tested in Experiment 1. }
    \label{fig:e1-conditions}
    \end{center}
\end{figure}

After learning about an experimenter-defined category, participants are asked to generate examples of a new category. Within this paradigm, an effect of category contrast would be realized if participants prefer to generate items in locations that are distant (i.e., perceptually dissimilar) from members of category A. However, generation is left unconstrained. Critically, participants were not asked to generate something different in the prompt. For example, participants assigned to the Cluster condition may generate a tightly clustered category in the corner opposite of the experimenter-defined category. Alternatively, they may generate a tightly clustered category directly overlapping with the experimenter-defined category. Further, they may even generate an entirely different type of category (e.g., a row category). 

In this way, we can analyze the results of the first experiment as a conceptual replication of the classic finding that generated categories tend to share distributional properties with known categories in the domain \citep[see][]{jern2013probabilistic,ward1994structured}. From these results, we can predict that, in each condition, participants should generate categories that are distributionally similar to the experimenter-defined category: In the Cluster condition, generated categories should be tightly clustered. In the Row condition, generated categories should very more along the X-axis than the Y-axis. In XOR condition, generated categories should be widely distributed across both dimensions, and the two dimensions should be positively correlated. However, if contrast plays a role, exemplars in the generated categories of participants in the XOR condition may not be positively correlated (they may even be negatively correlated instead!).

\nbcnote{I think that works as a decent motivation, do you think so?}

\jlanote{Very good}

% \jlanote{Is it possible to get the main individual differences we find in this study from different parameterizations of PACKER? If so, we can motivate it from there. Alternatively, we may want to switch the order of the two studies and motivate the experiment from the next one as a . I like switching the order of the experiments less because I feel like Experiment 2 nails contrast down and Experiment 1 is more showing how it plays a role in more complex category types}

\subsection{Participants \& Materials}

183 participants were recruited from Amazon Mechanical Turk. Participants were randomly assigned to one condition: 64 participants were assigned to the Cluster condition, 61 were assigned to the Row condition, and 58 were assigned to the XOR condition. Stimuli were squares varying in color (RGB 25--230) and side length (3.0--5.8cm). These stimuli are slight variants of those used by \cite{conaway2016similar}, see Figure \ref{fig:e1-conditions}. The assignment of perceptual features (color, size) to axes of the domain space (x, y) was counterbalanced across participants.

\jlanote{was the direction of each dimension also counterbalanced (as in Expt 2)?}

\subsection{Procedure}

Participants began the experiment with a short training phase (3 blocks of 4 trials), where they observed exemplars belonging to the `Alpha' category. Participants were instructed to learn as much as they can about the Alpha category, and that they would answer a series of test questions afterwards. On each trial, a single Alpha category exemplar was presented, and participants were given as much time as they desired before moving on. Each block consisted of a single presentation of each of the members of the Alpha category, in a random order. Participants were shown the range of possible colors and sizes prior to training.

Following the training phase, participants were asked to generate four examples belonging to another category called `Beta'. As in \citet{jern2013probabilistic}, generation was completed using a sliding-scale interface. Two scales controlled the features (color, size) of the generated example. An on-screen preview of the example updated whenever one of the features was changed. Participants could generate any example along an evenly-spaced 9x9 grid, except for any previously generated Beta exemplars. Neither the members of the Alpha category nor the previously generated Beta examples were visible during generation. Prior to beginning the generation phase, participants read the following instructions:

\begin{displayquote}
As it turns out, there is another category of geometric figures called ``Beta''. Instead of showing you examples of the Beta category, we would like to know what you think is likely to be in the Beta category. 

You will now be given the chance to create examples of any size or color in order to show what you expect about the Beta category. You will be asked to produce 4 Beta examples - they can be quite similar or quite different to each other, depending on what you think makes the most sense for the category.

Each example needs to be unique, but the computer will let you know if you accidentally create a repeat.
\end{displayquote}
\jlanote{were they allowed to create another Alpha or did those need to be unique too? we should be clear...}
%Following generation, participants completed a generalization phase wherein they classified novel examples into the Alpha and Beta categories without feedback. On each trial, a single example was presented, and participants were asked to classify it by clicking buttons labeled ``Alpha'' or ``Beta''. Participants classified a total of 81 items sampled along a 9x9 grid, including the members of the Alpha and Beta categories (randomly intermixed). These data were, however, collected to address a separate set of questions, and we do not discuss them in this paper.

\subsection{Results}

To begin, it is worth noting that we observed a substantial degree of individual differences in our data. In Figure \ref{fig:e1-samples} we have plotted sample data from several participants, from which it evident that different participants assumed qualitatively distinct approaches to category generation. In this section we will focus on what can be learned from analyzing the data in aggregate, but in later sections we will explore how individual differences can be explained.

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e1-samples.pgf}
    \caption{Sample categories generated in Experiment 1. }
    \label{fig:e1-samples}
    \nbcnote{I selected these at random, we can pick more representative samples later.}
    \jlanote{ok!}
    \end{center}
\end{figure}

To evaluate the role of contrast in category generation, we computed the number of times each stimulus was generated, as a function of its average city-block distance from members of the experimenter-defined ``Alpha'' category. These data, shown in Figure \ref{fig:e1-distanceplots}, reveal a clear pattern: examples that are more distant from members of the experimenter-defined categories are more likely to be generated into a new category. 

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e1-distanceplots.pgf}
    \caption{Experiment 1 results. \textit{Left}: Frequency of generation as a function of distance from members of the experimenter-defined category. \textit{Right}: Scatter plot of within-class versus between-class distance in each of the participant-generated categories.}

    \nbcnote{Need to check on how nice the journal is about color figs!}

    \label{fig:e1-distanceplots}
    \end{center}
\end{figure}


Figure \ref{fig:e1-distanceplots} also depicts, for each participant, the average amount of distance between members of the generated category (\textit{within-class} distance) against the average amount of distance between members of the generated and experimenter-defined category (\textit{between-class} distance). These data also reveal a systematic pattern: the majority of participants generated categories with more between-class distance than within-class distance. That is, members of the generated category tended to be more similar to one another than to members of the experimenter-defined category. To formally evaluate this claim, we conducted t-tests comparing the amount of within- and between- class distance in each condition. All conditions possessed greater between-class distance: Cluster, $t(63) = 11.43$, $p < 0.001$; Row, $t(60) = 13.16$, $p < 0.001$; and XOR, $t(57) = 3.64$, $p < 0.001$. These results provide clear evidence of an effect of category contrast: participants prefer to generate categories that are dissimilar to the learned category but maintain some level of internal cohesion. 

The secondary goal of this experiment was to examine whether we replicate the classic result that generated categories often possess the same distributional properties as previously-known categories. For each generated category, we computed the category range along each axis (X, Y), as well as the correlation between features. These data, shown in Figure \ref{fig:e1-statsboxes}, reveal broad individual differences: within each condition, participants generated categories spanning the entire X- and Y- axis as well as categories that spanned very little along each. Likewise, in each condition participants generated categories possessing strongly positive, neutral, and strongly negative correlations between the dimensions. Comparing the distributional statistics between conditions yields a broad-yet-incomplete replication of the classic effect. 

With respect to ranges along each axis (X, Y), the generated categories from each condition tend to reflect the ranges of the experimenter-defined categories. The categories generated in the Cluster condition were less widely distributed along the X-axis compared to Row, $t(123) = 5.61$, $p < 0.001$, and XOR, $t(120) = 2.68$, $p = 0.008$. Likewise, categories generated in the Row condition had less Y-axis range compared to Cluster, $t(123) = 4.57$, $p < 0.001$ and XOR $t(117) = 9.26$, $p < 0.001$, and categories from the Cluster condition had less Y-axis range compared to XOR, $t(120) = 3.95$, $p < 0.001$. However, whereas the correlations in the Cluster and Row conditions were not systematically positive or negative ($ps > 0.1$), the generated categories in the XOR condition tended to possess \textit{negatively} correlated dimensions, $t(57) = 2.04$, $p = 0.046$. This finding is notable, as it is the opposite of what would be expected, assuming learners are emulating the distributional structure of the experimenter-defined class (which possesses perfectly positively correlated features). Further it is expected by our proposal that contrast is a fundamental principle for category generation.

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e1-statsboxes.pgf}
    \caption{Box-plots of the distributional statistics from the categories generated in Experiment 1. }
    \jlanote{can you add a bit of an explanation of what boxplots are and how to interpret them? (short clearly)}
    \nbcnote{Weak spot: row has significantly \textit{less} X-range compared to XOR, which I do not know how to explain. For now I've ignored it.}
  	\jlanote{It's not really a weak spot for us rather that their results don't fully replicate in my opinion.}
    \label{fig:e1-statsboxes}
    \end{center}
\end{figure}


\subsection{Discussion}


In Experiment 1 we sought to test whether category contrast imposes constraints on creative generation. We found strong evidence for effects of category contrast: Participants were more likely to generate stimuli that are more distant from (i.e., less similar to) members of a previously-learned category, and members of participant-generated categories tended to be more similar to one another than to members of previously-learned categories. We also partially replicated the classic finding that the distributional structure of generated categories reflects that of previously learned categories \citep{jern2013probabilistic,ward1994structured}: members of generated categories were more widely distributed along dimensions which were widely distributed in the experimenter-defined category. 

Notably, however, we also found that participants who learned an XOR category (composed of exemplars following a positive diagonal, see Figure \ref{fig:e1-conditions}) tended to generate items according to a \textit{negative} feature correlation -- the opposite of what was present in the previously learned category. While this may be difficult to account for under existing theoretical approaches (which assume generated categories follow the same distributional structure as known categories), it can be concisely explained from a category contrast perspective. Specifically, within the XOR condition, individuals who seek to generate a category that is perceptually distinct from what is already known are left with only the upper-left and bottom-right quadrants of the space, as members of the previously-learned XOR category lie in the bottom-left and top-right. If examples are generated into both of the available quadrants, the generated category will possess a strongly negative correlation, opposing that of the experimenter-defined class.

Thus, while the core results of Experiment 1 indicate that generated categories systematically contrast with what is already known, the negative correlations observed in the XOR condition may signal something further. That is, the constraints on creative generation imposed by category contrast may not simply influence the \textit{location} of generated categories, but also their distributional structure. In Experiment 2, we test this claim more systematically.


\section{Experiment 2}

To test whether category contrast influences the distributional structure of generated categories, we repeated Experiment 1 using two new category types, depicted in Figure \ref{fig:e2-conditions}. The category types possess an identical distributional structure (both are tight clusters of examples with a neutral feature correlation), and only differ slightly in their Y-axis position: the `Bottom' category lies in the bottom-center of the space, and the `Middle' category lies in the center. This distributional equality of these conditions is key to the design of the experiment: if the structure of previously learned categories were the only influence on the structure of generated categories, we should observe no difference between these two conditions with respect to distributional structure.

\begin{figure}
    \begin{center}
    \input{figs/e2-conditions.pgf}
    \caption{Category types tested in Experiment 2.}
    \label{fig:e2-conditions}
    \end{center}
\end{figure}


Alternatively, participants seeking to make a perceptually distinct category would be more likely to distribute members of the generated category into areas that are distant from members of known categories. Thus, if category contrast influences the distributional structure of the categories people generate, then we should observe different types of categories according to the shape of the space that is unoccupied by members of previously learned categories. The difference in the Y-axis position between the Bottom and Middle types produces a considerable change to the shape of the unoccupied space: participants assigned to learn the Bottom category would be less likely to generate exemplars into the lower regions of the stimulus space (as these areas possess greater similarity to members of the Bottom category), preferring instead to distribute exemplars across the upper region of the space. This constraint is lifted in the Middle condition, as the Middle category exemplars are equidistant to the upper and lower regions of the space. Accordingly, participants should be more likely to utilize both of these areas.


\nbcnote{Weak point: this prediction is squishy-- why couldn't middle participants generate items above \textit{or} below? When we initially developed this, we were working with the pure-packing variant of PACKER, which is what predicted above-and-below for middle but not bottom. }

\jlanote{they could? wouldn't it depend on the extent that they care about within-category similarity?}

\subsection{Participants, Materials, \& Procedure}

122 participants were recruited from Amazon Mechanical Turk. 61 participants were randomly assigned to the Middle and Bottom conditions each. The stimuli were exactly as in Experiment 1. Again, the assignment of perceptual features (color, size) to axes of the domain space (x, y) was counterbalanced across participants. The procedure was exactly as in Experiment 1: participants first completed a short training phase, followed by the generation phase, followed by the generalization phase (data from this phase is not discussed in this report).


\subsection{Results}

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e2-samples.pgf}
    \caption{Sample categories generated in Experiment 2. }
    \label{fig:e2-samples}
    \end{center}
\end{figure}

As in Experiment 1, we observed broad differences in the generation approach taken by different participants.  To characterize the nature of these differences, Figure \ref{fig:e2-samples} depicts sample categories generated by participants. The data from each condition in Figure \ref{fig:e2-samples} is organized into four columns based on commonly observed patterns of generation: a `Cluster' type of tightly-clustered examples,   `Row' and `Column' types of exemplars widely distributed along the one axis but narrowly along the other, and a `Corners' type, wherein participants placed exemplars in disparate corners of the space. As before, in this section we focus on what can be concluded from analyzing the data in aggregate, but in later sections we will focus more specifically on explaining the individual differences.

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e2-distanceplots.pgf}
    \caption{Experiment 2 results. \textit{Left}: Frequency of generation as a function of distance from members of the experimenter-defined category. \textit{Right}: Scatter plot of within-class versus between-class distance in each of the participant-generated categories.}
    \nbcnote{May want to change condition colors if purple-orange is used as the colormap for the range heat-maps in the simulations section.}
    \jlanote{sounds good to me.}
    \label{fig:e2-distanceplots}
    \end{center}
\end{figure}

We began our analysis by again testing for the broad influence of category contrast on generation. As in Experiment 1, we computed the frequency each stimulus was generated as a function of its average distance from members of the experimenter defined category, as well as each participant's average within- and between- category distance. These data, shown in Figure \ref{fig:e2-distanceplots}, yield highly similar results. Stimuli that are more distant from members of the experimenter-defined category were more frequently generated, and the categories in each condition tended to possess more between-class than within-class distance: Bottom, $t(60) = 5.5$, $p < 0.001$; Middle, $t(60) = 2.71$, $p = 0.009$. 

We did, however, observe a notable subgroup of participants in each condition who generated categories with more within-class than between-class distance. Upon manual inspection, many of these individuals appear to have assumed a `Corners' strategy, placing exemplars in disparate corners of the space, thus producing much more within-class distance, see Figure \ref{fig:e2-samples} for examples.

\nbcnote{ Weak spot: the obvious next analysis here would involve the same box-and-whisker plot as in Experiment 1. We ran this analysis but the conditions did not differ in Y-range due to the data being weirdly distributed. But it feels odd to ignore this point, so I've included the box-and-whisker plot, as well as the graph-of-many-lines figure, on a separate page at the end.}

Because the conditions differ only in the Y-axis position of the experimenter-defined category, we then compared the conditions in terms of the frequency with which participants generated examples above and below the categories. Specifically, we counted the number of participants in each condition who placed at least one `Beta' exemplar on the top and bottom `rows' of the space (the maximum and minimum possible y-axis value, respectively). The resulting contingencies data are shown in Table \ref{table:e2-subset-table}. 

\begin{table}
\begin{center} 
\caption{Experiment 2 results.} 
\label{table:e2-subset-table} 
\vskip 0.12in
\begin{tabular}{ l r r}
    \textbf{Middle}         & Used top row & No top row \\
    \hline
    Used bottom row       &  28 & 18  \\
    No bottom row          &  11 &  4  \\
    \\
    \textbf{Bottom}         & Used top row & No top row \\
    \hline
    Used bottom row        & 16 & 8 \\
    No bottom row          & 31 & 6 \\
\end{tabular}
\end{center} 
\end{table}


Firstly, it should be noted that nearly every participant utilized the top and/or bottom rows: only $10 / 122$ participants generated their category entirely within the interior region. Fisher's Exact Tests comparing the conditions reveal that more Middle participants generated an exemplar in the bottom row, $p < 0.001$, again demonstrating the role of contrast guiding where exemplars are generated. The conditions did not differ in use of the top of the space, $p = 0.16$, however, more Middle participants placed exemplars in the top \textit{and} bottom rows, $p = 0.038$. The latter effect is of interest here, as it indicates that the shape of the unoccupied space exerts some influence on the distributional structure of generated categories: participants in the Middle condition were more likely to generate a category spanning the entire Y-axis.

\subsection{Discussion}

In Experiment 2, we replicated the core findings from Experiment 1: stimuli are more likely to be generated if they are distant from exemplars in other categories, and most participants generate categories with more between-class than within-class distance. However, we additionally found that the \textit{position} of a previously learned category (rather than its distributional structure) influences the types of categories people generate: participants who learned the `Middle' type were more likely to generate categories spanning the entire Y-axis of the space. Participants who learned the `Bottom' type were less likely to do so, presumably because of the presence of opposite category exemplars in the lower regions of the space.

This finding is cannot be explained under the theoretical perspective that the distributional structure of previously learned categories is the sole determinant of the distributional structure of generated categories. However, the observed performance is sensible under a category contrast perspective: participants seeking to generate a perceptually distinct category will be more likely to use areas of space that are unoccupied by exemplars belonging to previously learned categories. In the Middle condition, the upper and lower regions of space are equidistant from members of the experimenter-defined category, whereas in the Bottom condition, the lower region of the space is closer to members of the experimenter-defined category. Thus, while Middle participants may form categories around the use of the equally unoccupied areas, the same is not true for the Bottom condition.

\section{Model-based Analyses}

Experiments 1 and 2 revealed systematic and strong effects of category contrast on creative generation. In this section, we report the results of simulations with formal models aimed at explaining our observations. Specifically, we present simulations from the PACKER model, as well as a `copy-and-tweak' model (discussed in section \ref{section:copytweak-packer}), defined as a variant of PACKER with the $\phi$ and $\gamma$ parameters held constant. The comparison of these two models serves to highlight the explanatory role of contrast within PACKER's framework: if contrast affords little explanatory advantage, then the two accounts should produce an equally strong account. We also present simulations from an implementation of the hierarchical sampling model proposed by \cite{jern2013probabilistic}, described in-depth in Appendix \ref{ap:hsampling-definition}. The comparison between the hierarchical sampling model and PACKER is meant to emphasize the role of emulation of distributional structure: whereas PACKER is insensitive to the distributional structure of learned categories (relying only on within- and between-class similarity), the hierarchical sampling model generates categories exclusively on the basis of knowledge of how existing classes are distributed. Our approach in this section is to first broadly evaluate and compare the quality of each model's account to our entire dataset (Experiments 1 and 2 combined), then we more specifically describe the strengths and weakness of each model's account.

\subsection{Parameter-Fitting}

To obtain a global measure of the quality of each model's account, we fitted the parameters of each model to our entire dataset (Experiments 1 and 2 combined), using a hill-climbing algorithm which maximized the log-likelihood of the model's predictions of the observed responses (1220 responses from 305 total participants). We fitted four parameters in the PACKER model ($c$, $\phi$, $\gamma$, and $\theta$; see Section \ref{section:PACKER-definition}), as well as four in the hierarchical sampling model ($\kappa$, $\rho$, $\nu$, and $\theta$; see Appendix \ref{ap:hsampling-definition}). We fitted only two parameters for the copy-and-tweak model ($c$, and $\theta$), as $\phi$ and $\gamma$ are held constant. Note that each model possesses a $\theta$ parameter fulfilling the same role (response determinism). Attention ($w$, see Equation \ref{eq:similarity}) in PACKER and copy-and-tweak was set uniformly. Parameters were not allowed to vary between participants or conditions -- the goal was to obtain the best-fitting values to our entire dataset.

\begin{table}
\centering
\caption{Results of model-fitting to the combined datasets from Experiments 1 and 2.}
\label{table:global-model-fits}
\begin{tabular}{ l l l}
\\
 \textbf{PACKER}    & \textbf{Copy \& Tweak} & \textbf{Hierarchical Sampling} \\ 
 \hline
 $AIC = 9097$       & $AIC = 9842$          & $AIC = 10080$  \\ 
 $L = -4545$        & $L = -4919$           & $L = -5036$  \\ 
 $c=0.481$          & $c=3.189$             & $\kappa<0.001$\\
 $\phi=-0.945$      & $\phi=0$ (fixed)      & $\nu=1.001$ \\ 
 $\gamma=1.044$     & $\gamma=1$ (fixed)    & $\rho=5.313$  \\ 
 $\theta=3.354$     & $\theta=2.968$        & $\theta = 7.949$  \\ 
\end{tabular}
\end{table}

\jlanote{should do a nested model comparison with respect to PACKER v. copy \& tweak. they're nested, so should be doable...}


Table \ref{table:global-model-fits} contains the results of this fitting procedure. Due to the uneven number of fitted parameters among the models, we compare the model fits using the Akaike Information Criterion \citep[AIC;][]{akaike1974new}. Table \ref{table:global-model-fits} contains the AIC values of each model's best fit, as well as the corresponding log-likelihood ($L$) and the best-fitting parameter values. These results reveal strong model differentiation: the PACKER model achieved far better fits compared to the copy-and-tweak and hierarchical sampling models, and copy-and-tweak performed somewhat better than the hierarchical sampling model. While PACKER's advantage may tentatively be attributed to the model's sensitivity to category contrast (this will be explored in detail below), the advantage shown by copy-and-tweak over the hierarchical sampling model may be attributed to its exemplar-based representation, as opposed to the prototype-based representation assumed in the hierarchical sampling model. 
\nbcnote{That last point is a little squishy. We don't have `proof' that we can point to for skeptical readers, but that's clearly the issue: many categories were not normally distributed, and the copying of structure really only works out in 2/5 conditions, and even then there's so much variability...}
\jlanote{yeah, it's a small effect (in terms of $L$). }
\nbcnote{Should include info on fits for only trials 2-4.}

Through comparison with the copy-and-tweak model, Figure \ref{fig:packer-loglike} more clearly demonstrates the explanatory gains yielded by PACKER's sensitivity to between-class contrast. Plotted is PACKER's log-likelihood as a function of the model's between-category parameter, $\phi$. The models other parameters ($c$, $\gamma$, $\theta$) were set according to copy-and-tweak's best fits from Table \ref{table:global-model-fits}, and thus when $\phi=0$, the models are equivalent. The figure clearly shows a ``sweet spot'':  a convex region in which PACKER achieves superior fits as a result of changes to the $\phi$ parameter. The best fitting values lie between -0.6 and -0.8, well beyond the 0 value assumed by the copy-and-tweak model (though note PACKER achieves even better fits when its other parameters are fitted, as in Table \ref{table:global-model-fits}). Further, it demonstrates the robustness of the contrast effect.

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{packer-loglike.pgf}
    \caption{PACKER's fit as a function of the between-class contrast parameter ($\phi$). To facilitate comparison, PACKER's other parameters ($c$, $\gamma$, $\theta$) were set to the best fitting values obtained for copy-and-tweak in Table \ref{table:global-model-fits}. Thus, in this figure PACKER is equivalent to copy-and-tweak when $\phi = 0$.}
    \label{fig:packer-loglike}
    \end{center}
\end{figure}


\subsection{Individual Differences}

\section{General Discussion}






\clearpage
\section{Acknowledgments}
Previous versions of this work were presented at the Thirty-Ninth Annual Conference of the Cognitive Science Society and Forty-Ninth Annual Meeting of the Society for Mathematical Psychology. Support for this research was provided by the Office of the VCRGE at the UW - Madison with funding from the WARF. We thank Alan Jern and Charles Kemp for providing code and data.
\end{flushleft}


% references section
\clearpage
\bibliographystyle{apacite}
\bibliography{citations.bib}
\clearpage


\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e2-statsboxes.pgf}
    \caption{Box-plots of the distributional statistics from the categories generated in Experiment 2. }
    \nbcnote{Note the huge amount of variance in the range data, as well as the big Y-range difference in the medians.}

    \label{fig:e2-statsboxes}
    \end{center}
\end{figure}

\begin{figure}
    \begin{center}
    \inputpgf{figs/}{e2-yranges.pgf}
    \caption{Y-Axis range and position of the categories generated in Experiment 2. Each line corresponds to a single category, with notches corresponding to the Y-axis position of exemplars within the category. Data from each participant is sorted by overall range, and then by condition. }
    \label{fig:e2-yranges}
    \end{center}
\end{figure}



\clearpage

\appendix
\numberwithin{equation}{section}

\section{The Hierarchical Sampling Model}
\label{ap:hsampling-definition}

\cite{jern2013probabilistic} demonstrated how a hierarchical Bayesian model could explain the distributional correspondences between observed and generated categories. In their model, exemplars of generated category were viewed as samples from a multivariate Normal distribution over the dimensions of stimulus space. The mean of the generated category was independent of the observed categories, but the covariance matrix (encoding feature variances and correlations) was based on a common prior distribution. Generating a new category was thus completed by sampling a new category mean (uniform over stimulus space) and covariance matrix from the common prior distribution. Because the shared prior distribution's parameters were unobserved, the hierarchical Bayesian approach was used to infer its parameters from the previous categories (their feature variances and correlations), and then to generate the covariance matrix of the new category.

In our implementation of their model, each category's exemplars are assumed to be sampled from a multivariate Normal distribution with parameters ($\mu, \Sigma$). Each category's covariance matrix is assumed to be inverse-Wishart distributed with parameters ($v$, $\kappa,$ and $\Sigma_D$).\footnote{Note that \citet{jern2013probabilistic}'s model is slightly different, as they used a non-conjugate model. Their model acts very similar to our version of it and receives comparable fits.} $\Sigma_D$ is the covariance matrix shared between categories. We assume the shared covariance matrix $\Sigma_D$ is generated from a Wishart distribution (for conjugacy) with parameters $v_0$, $\kappa_0$, and $\Sigma_0$. We set $\nu_0 = 4$, and $\Sigma_0 = \rho {\bf I}$, where $\rho$ is a free parameter controlling the expected variance of dimensions (dimensions of the shared covariance matrix are expected to be uncorrelated) and ${\bf I}$ is the identity matrix.

\nbcnote{check on that $\nu_0 = 4$ business}

To simplify the model predictions, we used {\em maximum a posteriori} (MAP) estimates for the hidden parameters and then generated new categories based on those estimates. Due to conjugacy, the MAP estimate for the shared covariance matrix $\Sigma_D = \Sigma_0 + \sum_c{C_c}$, where $C_c$ is the empirical covariance matrix of category $c$. The MAP estimate of the covariance matrix for the target category $B$ is 
\begin{equation}
\Sigma_B = \left[ \Sigma_D \nu + C_B +
\dfrac
{\kappa n_B}
{\kappa + n_B}
(\bar{x}_B-\mu_B)(\bar{x}_B-\mu_B)^T
\right] (\nu + n_B)^{-1}
\label{eq:Sigma_B}
\end{equation}
%
where $\nu$ ($\nu>k-1$) is an additional free parameter (from the Inverse-Wishart prior on $\Sigma_B$) weighting the importance of $\Sigma_{D}$. When the target category has no members (i.e., $n_B = 0$), items are generated at random.

\nbcnote{should explain Equation \ref{eq:Sigma_B} more fully now that we are not limited for space.}

\jlanote{K, though if I remember, it's just conjugacy, in which case we can just cite a paper that has IW-W conjugacy.}

\nbcnote{It \textbf{is} just conjugacy, so we can just cite a paper (do you know of a paper or should I start digging?). We'll probably want to detail the what we did for priors and which free parameters we fitted though.}


Generated exemplars are drawn from a multivariate Normal distribution specified by $(\mu_{B}, \Sigma_{B})$. Thus, $p(y)$ is
\begin{equation}
p(y) = \dfrac
{\exp \left \{ \theta \cdot {\rm Normal}(y; \mu_{B}, \Sigma_{B}) \right \} }
{\sum_i \exp \left \{ \theta \cdot {\rm Normal}(y_i; \mu_{B}, \Sigma_{B}) \right \} } 
\end{equation}
where $\theta$ is a response determinism parameter and ${\rm Normal}(y; \mu, \Sigma)$ denotes a multivariate Normal density evaluated at $y$. 


\end{document}
